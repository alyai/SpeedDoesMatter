@{
    ViewData["Title"] = "About Pagespeed";
}
    <h1>@ViewData["Title"] </h1>


<p>Use this page to detail your site's privacy policy.</p>
<p>

    PageSpeed Insights (PSI) reports on the performance of a page on both mobile and desktop
    devices, and provides suggestions on how that page may be improved.
</p>

<p>

    PSI provides both lab and field data about a page. Lab data is useful for debugging
    performance issues, as it is collected in a controlled environment. However, it may not
    capture real-world bottlenecks. Field data is useful for capturing true, real-world user
    experience - but has a more limited set of metrics. See
    <a href="/web/fundamentals/performance/speed-tools">
        How To Think
        About Speed Tools
    </a>
    for more information on the 2 types of data.

</p>
<h2 id="score" data-text="Performance score" tabindex="0">Performance score</h2>

<p>

    At the top of the report, PSI provides a score which summarizes the page’s performance.
    This score is determined by running
    <a href="/web/tools/lighthouse">Lighthouse</a>
    to collect and
    analyze
    <a href="#lab">lab data</a>
    about the page. A score of 90 or above is considered good.
    50 to 90 is a score that needs improvement, and below 50 is considered poor.

</p>
<h2 id="crux" data-text="Real-World Field Data" tabindex="0">Real-World Field Data</h2>

<p>

    When PSI is given a URL, it will look it up in the
    <a href="/web/tools/chrome-user-experience-report">
        Chrome User Experience
        Report
    </a>
    (CrUX) dataset. If available, PSI reports the
    <a href="https://web.dev/fcp/">
        First
        Contentful Paint
    </a>
    (FCP),
    <a href="https://web.dev/fid/">First Input Delay</a>
    (FID),

    <a href=https://web.dev/lcp />Largest Contentful Paint</a>
    (LCP), and
    <a href=https://web.dev/cls />Cumulative Layout
    Shift</a>
    (CLS) metric data for the origin and potentially the specific page URL.

</p>
<h2 id="categories" data-text="Classifying Good, Needs Improvement, Poor" tabindex="0">Classifying Good, Needs Improvement, Poor</h2>
<p>
    PSI also classifies field data into 3 buckets, describing experiences deemed good, needs
    improvement, or poor. PSI sets the following thresholds for good / needs improvement / poor,
    based on our analysis of the CrUX dataset:
</p>
<table>
    <tr>
        <th></th>
        <th>Good</th>
        <th>Needs Improvement</th>
        <th>Poor</th>
    </tr>
    <tr>
        <td>FCP</td>
        <td>[0, 1000ms]</td>
        <td>(1000ms, 3000ms]</td>
        <td>over 3000ms</td>
    </tr>
    <tr>
        <td>FID</td>
        <td>[0, 100ms]</td>
        <td>(100ms, 300ms]</td>
        <td>over 300ms</td>
    </tr>
    <tr>
        <td>LCP</td>
        <td>[0, 2500ms]</td>
        <td>(2500ms, 4000ms]</td>
        <td>over 4000ms</td>
    </tr>
    <tr>
        <td>CLS</td>
        <td>[0, 0.1]</td>
        <td>(0.1, 0.25]</td>
        <td>over 0.25</td>
    </tr>
</table>
<h3 id="distribution" data-text="Distribution and selected metric values" tabindex="0">Distribution and selected metric values</h3>
<p>
    PSI presents a distribution of these metrics so that developers can understand the
    range of FCP, FID, LCP, and CLS values for that page or origin. This distribution is also
    split into three categories: Good, Needs Improvement, and Poor, denoted with green, orange,
    and red bars. For example, seeing 14% within FCP's orange bar indicates that 14% of all
    observed FCP values fall between 1000ms and 3000ms. This data represents an aggregate view
    of all page loads over the previous 28-day collection period.
</p>
<p>

    Above the distribution bars, PSI reports the 75th percentile for all metrics.
    The 75th percentile is
    <a href="#faq">selected</a>
    so that developers can understand the
    most frustrating user experiences on their site. These field metric values are classified
    as good/needs improvement/poor by applying the same thresholds shown above.

</p>
<h3 id="field-data-label" data-text="Core Web Vitals" tabindex="0">Core Web Vitals</h3>

<p>
    <img src="/speed/docs/insights/images/v5/blueribbon.svg" alt="" style="vertical-align: text-top;">
    <a href="https://web.dev/vitals/">Core Web Vitals</a>
    are a common set of signals critical to all web
    experiences. The Core Web Vitals metrics are FID, LCP, and CLS, with their respective
    thresholds. A page passes the Core Web Vitals assessment if the 75th percentiles of
    all three metrics are Good. Otherwise, the page does not pass the assessment.

</p>
<h3 id="differences" data-text="Differences between Field Data in PSI and CrUX" tabindex="0">Differences between Field Data in PSI and CrUX</h3>
<p>
    The difference between the field data in PSI versus the Chrome User Experience Report on
    BigQuery, is that PSI’s data is updated daily for the trailing 28-day period. The data set on
    BigQuery is only updated monthly.
</p>
<h2 id="lab" data-text="Lab data" tabindex="0">Lab data</h2>

<p>

    PSI uses Lighthouse to analyze the given URL, generating a performance score that estimates
    the page's performance on different metrics, including:
    <a href="https://web.dev/fcp/">
        First Contentful
        Paint
    </a>
    ,
    <a href="https://web.dev/lcp/">Largest Contentful Paint</a>
    ,
    <a href="https://web.dev/speed-index/">
        Speed
        Index
    </a>
    ,
    <a href="https://web.dev/cls/">Cumulative Layout Shift</a>
    ,
    <a href="https://web.dev/interactive/">
        Time to
        Interactive
    </a>
    , and
    <a href="https://web.dev/tbt/">Total Blocking Time</a>
    .

</p>

<p>
    Each metric is
    <a href="https://web.dev/performance-scoring/">scored</a>
    and labeled with a icon:
</p>
<ul>
    <li>Good is indicated with a green check mark</li>
    <li>Needs Improvement is indicated with orange informational circle</li>
    <li>Poor is indicated with a red warning triangle</li>
</ul>
<h2 id="audits" data-text="Audits" tabindex="0">Audits</h2>
<p>Lighthouse separates its audits into three sections:</p>
<ul>
    <li>
        <b>Opportunities</b>
        provide suggestions how to improve the page’s performance metrics.
        Each suggestion in this section estimates how much faster the page will load if the
        improvement is implemented.

    </li>
    <li>
        <b>Diagnostics</b>
        provide additional information about how a page adheres to best
        practices for web development.

    </li>
    <li>
        <b>Passed Audits</b>
        indicates the audits that have been passed by the page.

    </li>
</ul>
<h2 id="faq" data-text="Frequently asked questions (FAQs)" tabindex="0">Frequently asked questions (FAQs)</h2>
<p>
    <b>What device and network conditions does Lighthouse use to simulate a page load?</b>
</p>

<p>

    Currently, Lighthouse simulates a page load on a mid-tier device (Moto G4) on a

    <a class="external" href="https://github.com/GoogleChrome/lighthouse/blob/master/docs/throttling.md">mobile network</a>
    .

</p>
<p>
    <b>
        Why do the field data and lab data sometimes contradict each other?
    </b>
</p>
<p>
    The field data is a historical report about how a particular URL has performed, and represents
    anonymized performance data from users in the real-world on a variety of devices and network
    conditions. The lab data is based on a simulated load of a page on a single device and fixed
    set of network conditions. As a result, the values may differ.
</p>
<p>
    <b>
        Why is the 75th percentile chosen for all metrics?
    </b>
</p>
<p>
    Our goal is to make sure that pages work well for the majority of users. By focusing on 75th
    percentile values for our metrics, this ensures that pages provide a good user experience
    under the most difficult device and network conditions.
</p>
<p>
    <b>
        Why does the FCP in v4 and v5 have different values?
    </b>
</p>

                                      FCP in v5 reports the 75th percentile (as of November 4th 2019), previously it was the 90th percentile.
                                      In v4, FCP reports the median (50th percentile).

<p></p>
<p>
    <b>
        Why does the FID in v5 have different values?
    </b>
</p>

                                      FID reports the 75th percentile (as of May 27th 2020), previously it was the 95th percentile.

<p></p>
<p>
    <b>
        What is a good score for the lab data?
    </b>
</p>
<p>
    Any green score (90+) is considered good.
</p>
<p>
    <b>
        Why does the performance score change from run to run? I didn’t change anything on my page!
    </b>
</p>
<p>
    Variability in performance measurement is introduced via a number of channels with different
    levels of impact. Several common sources of metric variability are local network availability,
    client hardware availability, and client resource contention.
</p>
<p>
    <b>
        Why is the real-world Chrome User Experience Report speed data not available for a URL?
    </b>
</p>

<p>

    Chrome User Experience Report aggregates real-world speed data from

    <a target="_blank" rel="noopener" href="https://developers.google.com/web/tools/chrome-user-experience-report/#methodology">opted-in users</a>
    and
    requires that a URL must be public
    (
    <a target="_blank" rel="noopener" href="https://developers.google.com/web/tools/lighthouse/audits/indexing">crawlable and indexable</a>
    )
    and have sufficient number of distinct samples that provide a representative, anonymized view
    of performance of the URL.

</p>
<p>
    <b>
        Why is the real-world Chrome User Experience Report speed data not available for an origin?
    </b>
</p>
<p>

    Chrome User Experience Report aggregates real-world speed data from

    <a target="_blank" rel="noopener" href="https://developers.google.com/web/tools/chrome-user-experience-report/#methodology">opted-in users</a>
    and
    requires that an origin's root page must be public
    (
    <a target="_blank" rel="noopener" href="https://developers.google.com/web/tools/lighthouse/audits/indexing">crawlable and indexable</a>
    )
    and have sufficient number of distinct samples that provide a representative, anonymized view
    of origin’s performance across all URLs that are visited on that origin.

</p>